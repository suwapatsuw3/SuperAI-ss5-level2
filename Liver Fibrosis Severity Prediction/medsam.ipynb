{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea3be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "join = os.path.join\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import monai\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('utils'))\n",
    "# from SurfaceDice import compute_dice_coefficient\n",
    "# set seeds\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab09a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice_coefficient(pred_mask: np.ndarray, true_mask: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient.\n",
    "    Args:\n",
    "        pred_mask (np.ndarray): Predicted binary mask (0 or 1)\n",
    "        true_mask (np.ndarray): Ground truth binary mask (0 or 1)\n",
    "    Returns:\n",
    "        float: Dice score\n",
    "    \"\"\"\n",
    "    pred_mask = pred_mask.astype(bool)\n",
    "    true_mask = true_mask.astype(bool)\n",
    "    \n",
    "    intersection = np.logical_and(pred_mask, true_mask).sum()\n",
    "    total = pred_mask.sum() + true_mask.sum()\n",
    "    \n",
    "    if total == 0:\n",
    "        return 1.0  # Both masks are empty\n",
    "    return 2.0 * intersection / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c9f5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc87cc79a30>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/surfacedice/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc87cc797c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/surfacedice/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc87cc82a30>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/surfacedice/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc87cc82be0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/surfacedice/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fc87cc82d90>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /simple/surfacedice/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement SurfaceDice (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for SurfaceDice\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SurfaceDice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall SurfaceDice\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSurfaceDice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_dice_coefficient\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SurfaceDice'"
     ]
    }
   ],
   "source": [
    "%pip install SurfaceDice\n",
    "from SurfaceDice import compute_dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159ce33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% create a dataset class to load npz data and return back image embeddings and ground truth\n",
    "class NpzDataset(Dataset): \n",
    "    def __init__(self, data_root):\n",
    "        self.data_root = data_root\n",
    "        self.npz_files = sorted(os.listdir(self.data_root)) \n",
    "        self.npz_data = [np.load(join(data_root, f)) for f in self.npz_files]\n",
    "        # this implementation is ugly but it works (and is also fast for feeding data to GPU) if your server has enough RAM\n",
    "        # as an alternative, you can also use a list of npy files and load them one by one\n",
    "        self.ori_gts = np.vstack([d['gts'] for d in self.npz_data])\n",
    "        self.img_embeddings = np.vstack([d['img_embeddings'] for d in self.npz_data])\n",
    "        print(f\"{self.img_embeddings.shape=}, {self.ori_gts.shape=}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.ori_gts.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_embed = self.img_embeddings[index]\n",
    "        gt2D = self.ori_gts[index]\n",
    "        y_indices, x_indices = np.where(gt2D > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = gt2D.shape\n",
    "        x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "        # convert img embedding, mask, bounding box to torch tensor\n",
    "        return torch.tensor(img_embed).float(), torch.tensor(gt2D[None, :,:]).long(), torch.tensor(bboxes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb5822c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  5 08:58:56 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    60W / 400W |   4751MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     36241      C   ...ightning-2.2.5/bin/python     4748MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print(subprocess.getoutput(\"nvidia-smi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4ce7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting MedSAM embeddings: 100%|██████████| 2219/2219 [11:37<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "# === CONFIG ===\n",
    "image_dir = \"/project/ai901504-ai0004/kaggle_competition/week5/liver-fibrosis-severity-prediction/images/images\"\n",
    "output_dir = \"/project/ai901504-ai0004/507a/week5/medsam_embeddings/train\"\n",
    "model_dir = \"/project/ai901504-ai0004/507a/week5/medsam-vit-base/models--flaviagiammarino--medsam-vit-base/snapshots/a3cb4c518fc3ae8beaf1af0cd3a43867cfa28335\"  # adjust if different\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SamModel.from_pretrained(model_dir).to(device)\n",
    "processor = SamProcessor.from_pretrained(model_dir)\n",
    "\n",
    "image_list = [f for f in os.listdir(image_dir) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "for image_name in tqdm(image_list, desc=\"Extracting MedSAM embeddings\"):\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    image_embedding = model.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "    image_embedding = image_embedding.squeeze(0).cpu().numpy()  # shape: (256, 64, 64)\n",
    "\n",
    "    # Save as .npz\n",
    "    npz_name = os.path.splitext(image_name)[0] + \".npz\"\n",
    "    output_path = os.path.join(output_dir, npz_name)\n",
    "    np.savez_compressed(output_path, embedding=image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93120421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 .npz files: ['5c7de4d6e138234adf00044d.npz', '5c7e1166e138234adf000b5a.npz', '5c809dc9e138233fae00064d.npz']\n",
      "Keys in sample file: ['embedding']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "npz_path = \"/project/ai901504-ai0004/507a/week5/medsam_embeddings/train\"\n",
    "files = sorted(os.listdir(npz_path))\n",
    "print(\"First 3 .npz files:\", files[:3])\n",
    "\n",
    "# Load and inspect one .npz file\n",
    "sample = np.load(os.path.join(npz_path, files[0]))\n",
    "print(\"Keys in sample file:\", sample.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b3a5349",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'gts is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m npz_tr_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/project/ai901504-ai0004/507a/week5/medsam_embeddings/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(npz_tr_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m demo_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNpzDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpz_tr_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m demo_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(demo_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_embed, gt2D, bboxes \u001b[38;5;129;01min\u001b[39;00m demo_dataloader:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# img_embed: (B, 256, 64, 64), gt2D: (B, 1, 256, 256), bboxes: (B, 4)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36mNpzDataset.__init__\u001b[0;34m(self, data_root)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_data \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(join(data_root, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_files]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# this implementation is ugly but it works (and is also fast for feeding data to GPU) if your server has enough RAM\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# as an alternative, you can also use a list of npy files and load them one by one\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mori_gts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_data])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_data])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mori_gts\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_data \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(join(data_root, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_files]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# this implementation is ugly but it works (and is also fast for feeding data to GPU) if your server has enough RAM\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# as an alternative, you can also use a list of npy files and load them one by one\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mori_gts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_data])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpz_data])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_embeddings\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mori_gts\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lustrefs/disk/modules/easybuild/software/Mamba/23.11.0-0/envs/lightning-2.2.5/lib/python3.9/site-packages/numpy/lib/npyio.py:263\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a file in the archive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gts is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "# %% test dataset class and dataloader\n",
    "npz_tr_path = '/project/ai901504-ai0004/507a/week5/medsam_embeddings/train'\n",
    "os.makedirs(npz_tr_path, exist_ok=True)\n",
    "demo_dataset = NpzDataset(npz_tr_path)\n",
    "demo_dataloader = DataLoader(demo_dataset, batch_size=8, shuffle=True)\n",
    "for img_embed, gt2D, bboxes in demo_dataloader:\n",
    "    # img_embed: (B, 256, 64, 64), gt2D: (B, 1, 256, 256), bboxes: (B, 4)\n",
    "    print(f\"{img_embed.shape=}, {gt2D.shape=}, {bboxes.shape=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a190f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% set up model for fine-tuning \n",
    "# train data path\n",
    "npz_tr_path = 'data/demo2D_vit_b'\n",
    "work_dir = './work_dir'\n",
    "task_name = 'demo2D'\n",
    "# prepare SAM model\n",
    "model_type = 'vit_b'\n",
    "checkpoint = 'work_dir/SAM/sam_vit_b_01ec64.pth'\n",
    "device = 'cuda:0'\n",
    "model_save_path = join(work_dir, task_name)\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to(device)\n",
    "sam_model.train()\n",
    "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6568328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "best_loss = 1e10\n",
    "train_dataset = NpzDataset(npz_tr_path)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    # train\n",
    "    for step, (image_embedding, gt2D, boxes) in enumerate(tqdm(train_dataloader)):\n",
    "        # do not compute gradients for image encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            # convert box to 1024x1024 grid\n",
    "            box_np = boxes.numpy()\n",
    "            sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "            box = sam_trans.apply_boxes(box_np, (gt2D.shape[-2], gt2D.shape[-1]))\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            if len(box_torch.shape) == 2:\n",
    "                box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "            # get prompt embeddings \n",
    "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box_torch,\n",
    "                masks=None,\n",
    "            )\n",
    "        # predicted masks\n",
    "        mask_predictions, _ = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding.to(device), # (B, 256, 64, 64)\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          )\n",
    "\n",
    "        loss = seg_loss(mask_predictions, gt2D.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= step\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'EPOCH: {epoch}, Loss: {epoch_loss}')\n",
    "    # save the latest model checkpoint\n",
    "    torch.save(sam_model.state_dict(), join(model_save_path, 'sam_model_latest.pth'))\n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(sam_model.state_dict(), join(model_save_path, 'sam_model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fed0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plt.plot(losses)\n",
    "plt.title('Dice + Cross Entropy Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show() # comment this line if you are running on a server\n",
    "plt.savefig(join(model_save_path, 'train_loss.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compare the segmentation results between the original SAM model and the fine-tuned model\n",
    "# load the original SAM model\n",
    "from skimage import io\n",
    "ori_sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to(device)\n",
    "ori_sam_predictor = SamPredictor(ori_sam_model)\n",
    "\n",
    "ts_img_path = 'data/MedSAMDemo_2D/test/images'\n",
    "ts_gt_path = 'data/MedSAMDemo_2D/test/labels'\n",
    "test_names = sorted(os.listdir(ts_img_path))\n",
    "\n",
    "# random select a test case\n",
    "img_idx = np.random.randint(len(test_names))\n",
    "image_data = io.imread(join(ts_img_path, test_names[img_idx]))\n",
    "if image_data.shape[-1]>3 and len(image_data.shape)==3:\n",
    "    image_data = image_data[:,:,:3]\n",
    "if len(image_data.shape)==2:\n",
    "    image_data = np.repeat(image_data[:,:,None], 3, axis=-1)\n",
    "# read ground truth (gt should have the same name as the image) and simulate a bounding box\n",
    "def get_bbox_from_mask(mask):\n",
    "    '''Returns a bounding box from a mask'''\n",
    "    y_indices, x_indices = np.where(mask > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # add perturbation to bounding box coordinates\n",
    "    H, W = mask.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "\n",
    "    return np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "gt_data = io.imread(join(ts_gt_path, test_names[img_idx]))\n",
    "bbox_raw = get_bbox_from_mask(gt_data)\n",
    "\n",
    "\n",
    "# preprocess: cut-off and max-min normalization\n",
    "lower_bound, upper_bound = np.percentile(image_data, 0.5), np.percentile(image_data, 99.5)\n",
    "image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
    "image_data_pre = (image_data_pre - np.min(image_data_pre))/(np.max(image_data_pre)-np.min(image_data_pre))*255.0\n",
    "image_data_pre[image_data==0] = 0\n",
    "image_data_pre = np.uint8(image_data_pre)\n",
    "H, W, _ = image_data_pre.shape\n",
    "\n",
    "# predict the segmentation mask using the original SAM model\n",
    "ori_sam_predictor.set_image(image_data_pre)\n",
    "ori_sam_seg, _, _ = ori_sam_predictor.predict(point_coords=None, box=bbox_raw, multimask_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a614bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the segmentation mask using the fine-tuned model\n",
    "# resize image to 3*1024*1024\n",
    "sam_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "resize_img = sam_transform.apply_image(image_data_pre)\n",
    "resize_img_tensor = torch.as_tensor(resize_img.transpose(2, 0, 1)).to(device)\n",
    "input_image = sam_model.preprocess(resize_img_tensor[None,:,:,:]) # (1, 3, 1024, 1024)\n",
    "assert input_image.shape == (1, 3, sam_model.image_encoder.img_size, sam_model.image_encoder.img_size), 'input image should be resized to 1024*1024'\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pre-compute the image embedding\n",
    "    ts_img_embedding = sam_model.image_encoder(input_image)\n",
    "    # convert box to 1024x1024 grid\n",
    "    bbox = sam_trans.apply_boxes(bbox_raw, (H, W))\n",
    "    print(f'{bbox_raw=} -> {bbox=}')\n",
    "    box_torch = torch.as_tensor(bbox, dtype=torch.float, device=device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :] # (B, 4) -> (B, 1, 4)\n",
    "    \n",
    "    sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    medsam_seg_prob, _ = sam_model.mask_decoder(\n",
    "        image_embeddings=ts_img_embedding.to(device), # (B, 256, 64, 64)\n",
    "        image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "        )\n",
    "    medsam_seg_prob = torch.sigmoid(medsam_seg_prob)\n",
    "    # convert soft mask to hard mask\n",
    "    medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "    medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "    print(medsam_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d56cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_sam_dsc = compute_dice_coefficient(gt_data>0, ori_sam_seg>0)\n",
    "medsam_dsc = compute_dice_coefficient(gt_data>0, medsam_seg>0)\n",
    "print('Original SAM DSC: {:.4f}'.format(ori_sam_dsc), 'MedSAM DSC: {:.4f}'.format(medsam_dsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% visualize the segmentation results of the middle slice\n",
    "# visualization functions\n",
    "# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "# change color to avoid red and green\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))    \n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=(25, 25))\n",
    "axs[0].imshow(image_data)\n",
    "show_mask(gt_data>0, axs[0])\n",
    "# show_box(box_np[img_id], axs[0])\n",
    "# axs[0].set_title('Mask with Tuned Model', fontsize=20)\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(image_data)\n",
    "show_mask(ori_sam_seg, axs[1])\n",
    "show_box(bbox_raw, axs[1])\n",
    "# add text to image to show dice score\n",
    "axs[1].text(0.5, 0.5, 'SAM DSC: {:.4f}'.format(ori_sam_dsc), fontsize=30, horizontalalignment='left', verticalalignment='top', color='yellow')\n",
    "# axs[1].set_title('Mask with Untuned Model', fontsize=20)\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(image_data)\n",
    "show_mask(medsam_seg, axs[2])\n",
    "show_box(bbox_raw, axs[2])\n",
    "# add text to image to show dice score\n",
    "axs[2].text(0.5, 0.5, 'MedSAM DSC: {:.4f}'.format(medsam_dsc), fontsize=30, horizontalalignment='left', verticalalignment='top', color='yellow')\n",
    "# axs[2].set_title('Ground Truth', fontsize=20)\n",
    "axs[2].axis('off')\n",
    "plt.show()  \n",
    "plt.subplots_adjust(wspace=0.01, hspace=0)\n",
    "# save plot\n",
    "# plt.savefig(join(model_save_path, test_npzs[npz_idx].split('.npz')[0] + str(img_id).zfill(3) + '.png'), bbox_inches='tight', dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
